model_name: meta-llama/Llama-3-8B-Instruct
output_dir: outputs/qlora-llama3-8b
qlora:
  load_8bit: true
  r: 16
  alpha: 32
  dropout: 0.05
training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 2e-4
  max_steps: 150
  fp16: true
data:
  train_file: data/demo.jsonl
  text_field: "text"
  cutoff_len: 2048
