model_name: meta-llama/Llama-3-8B-Instruct
output_dir: outputs/lora-llama3-8b
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj"]
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  num_train_epochs: 1
  max_steps: 100
  logging_steps: 10
  save_steps: 50
  fp16: true
data:
  train_file: data/demo.jsonl
  text_field: "text"
  cutoff_len: 2048
